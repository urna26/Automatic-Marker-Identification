{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Marker auto-detection\n",
    "#### Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data loading and plotting packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model metric packages \n",
    "#from sklearn import cross_validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,roc_curve, auc\n",
    "from sklearn import metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text pre-processing packages \n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the root data into the system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_data=pd.read_excel(\"data path\")\n",
    "root_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set(root_data)\n",
    "\n",
    "root_data[\"target var\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removing the missing values from free format description\n",
    "\n",
    "root_data=root_data[root_data[\"Free format Desc\"].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing the records of 'Natural hazard', 'Theft' and \"Flood\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing records which has srong words  in \"Free format Desc\" column\n",
    "\n",
    "#Strong words-If these words are present then it is total loss \n",
    "strong_words=[evident words which could be generalised]\n",
    "\n",
    "# Removing the strong word records in \"Free format Desc\" column\n",
    "root_data1=root_data1[~root_data1['Free format Desc'].str.contains('|'.join(strong_words),case=False)]\n",
    "print(\"Number of records removed based on strong words are \",root_data.shape[0]-root_data1.shape[0])\n",
    "\n",
    "# Reset and drop the index \n",
    "root_data1.reset_index(inplace=True,drop=True)\n",
    "\n",
    "# Result:Number of records removed based on strong words are  137"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -----------------------------------------------------------------Text Data  Pre-processing-----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Removal of punctuations and special characteristics and handling case senitivity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Removal of Special Characters and Punctuations \n",
    "root_data1['Free_format_Desc'] = root_data1['Free format Desc'] .str.replace('[^\\w\\s]','')\n",
    "\n",
    "# converting into lower case \n",
    "root_data1['Free_format_Desc'] = root_data1['Free_format_Desc'].apply(lambda x: \" \".join(x.lower() for x in x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Elaborating Abbrevations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class for replace the Abbrevations and spell corrections using word map \n",
    "class WordReplacer(object): \n",
    " \n",
    "    def __init__(self, word_map):\n",
    "            self.word_map = word_map \n",
    "    def replace(self, word): \n",
    "         return self.word_map.get(word, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to replace abbrevations and spell corrections \n",
    "def replace_text(text):\n",
    "    tokenized_text = word_tokenize(text.lower()) \n",
    "    replaced_text=[replacer.replace(word) for word in tokenized_text ]\n",
    "    replaced_text =' '.join(replaced_text)\n",
    "    return replaced_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word map to handle all the abbrevations and spell corrections that occured in metrics in textual data \n",
    "wordmap={\n",
    "# Abbrevations labelling\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replacing observations and spell corrections on \"Free_format_updated\" column\n",
    "replacer=WordReplacer(wordmap)\n",
    "root_data1['Free_format_Desc']=root_data1['Free_format_Desc'].apply(replace_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Manual corrections of words which occured in metrics created from text data \n",
    "root_data1['Free_format_Desc_Updated'] = root_data1['Free_format_Desc_Updated'] .str.replace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Tokenazation and Removal of Stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of customized stopwords are : 349\n"
     ]
    }
   ],
   "source": [
    "# Sysytem defined stop-words-(Eliminated all negated stopwords(Eg: off,over,doesn't,not etc.,))\n",
    "#sample structure\n",
    "stop_words=stopwords.words('english')\n",
    "rm_stop=['before', 'after', 'above', 'below', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'no',\n",
    "         'nor', 'not', 'too', 'very', \"don't\", \"aren't\", \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", \"hadn't\", \"hasn't\",\n",
    "          \"haven't\", \"wasn't\", \"weren't\"]\n",
    "stop_words= [word for word in stop_words if word  not in rm_stop]\n",
    "\n",
    "# user defined stopwords(based on 1 gram analysis)\n",
    "stop_words.extend(['number', 'plate','paint','presume','pour','car','vehicle','tp','policyholder','policy holder','ph','phv','insured','policy','holder',\n",
    "                   #stopwords from unigrams\n",
    "                   'tbc','go','ab','tl','ncrc','ahs','fr','cv','nar','ne','xs','lr',\n",
    "\n",
    "                   # Stopwords taken from TFIDF \n",
    "'a','also','although','always','am','among','amongst','amoungst','amount','an','and','another','any','anyhow','anyone',\n",
    "'anything','anyway','anywhere','are','around','as','at','be','became','because','become','becomes','becoming','been','being',\n",
    "'between','bill','but','by','call','cry','de','describe','due','during','each','eg','either','eleven','else','even','ever',\n",
    "'fifteen','fifty','for','former','formerly','forty','from','further','give','go','here','hereafter','hereby','herein','hereupon',\n",
    "'hers','him','his','how','however','i','ie','if','inc','interest','is','it','its','itself','latter','latterly','ltd','made',\n",
    "'may','me','meanwhile','might','mill','mine','my','myself','name','namely','nine','no','noone','now','often','once','our',\n",
    "'perhaps','rather','re','same','see','seem','seemed','seeming','seems','she','show','since','sincere','six','sixty','so',\n",
    "'sometime','sometimes','somewhere','still','such','system','take','ten','that','the','their','them','themselves','then',\n",
    "'thence','there','thereafter','thereby','therefore','therein','thereupon','these','they','third','this','those','thus','twelve',\n",
    "'twenty','un','until','us','was','we','were','what','whatever','when','whence','whenever','where','whereafter','whereas','whereby',\n",
    "'wherein','whereupon','wherever','whether','which','who','whoever','whom','whose','why','will','would','you','your','yours',\n",
    "'yourself','yourselves'])\n",
    "\n",
    "print(\"Total number of customized stopwords are :\",len(stop_words))\n",
    "\n",
    "# Result: Total number of customized stopwords are : 337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to replace stopwords \n",
    "def clean_text(text):\n",
    "   \n",
    "    # Tokenization of text\n",
    "    tokenized_text = word_tokenize(text.lower()) \n",
    "    \n",
    "    # Removal of Stop words \n",
    "    cleaned_text =[t for t in tokenized_text if t not in stop_words and re.match('[a-zA-Z\\\\-][a-zA-Z\\\\-]{1,}', t) and t.isalpha()]\n",
    "    #Join all words\n",
    "    cleaned_text =' '.join(cleaned_text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replacing Stopwords in \"Free_format_updated\" column\n",
    "root_data1['Free_format_Desc']=root_data1['Free_format_Desc'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text before stopword removal and handling abbrevations \n",
    "root_data1['Free format Desc'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text after stopword removal \n",
    "root_data1['Free_format_Desc_Updated'].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Lemmatization of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to perform Lematization\n",
    "lmtzr = WordNetLemmatizer()\n",
    "def clean_text_lemma(text):\n",
    "        \n",
    "    # Tokenization of text\n",
    "    tokenized_text = word_tokenize(text.lower()) \n",
    "    \n",
    "    # Lemmatization\n",
    "    cleaned_text =[lmtzr.lemmatize(t,pos='v') for t in tokenized_text]\n",
    "    \n",
    "    #Joining all words\n",
    "    cleaned_text =' '.join(cleaned_text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Performing lematization on \"Free_format_updated\" column\n",
    "root_data1['Free_format_Desc']=root_data1['Free_format_Desc'].apply(clean_text_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data1['Free_format_Desc_Updated'].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 5 :Removing the Records of strong words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removing the strong word records in \"free flow text\" column\n",
    "root_data1=root_data1[~root_data1['Free_format_Desc'].str.contains('|'.join(strong_words),case=False)]\n",
    "root_data1.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### -----------------------------------------------------------------------End of Text Data  Pre-Processing------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
